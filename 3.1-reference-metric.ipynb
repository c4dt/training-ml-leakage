{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494e9cc6",
   "metadata": {},
   "source": [
    "# Measuring reference metric\n",
    "\n",
    "This notebook shows how to calculate a more detailed metric, the reference metric.\n",
    "For this metric, it is necessary to create many models to calculate the probability that\n",
    "a given case will be recognized as being in the final model or not.\n",
    "When using `RandomForestClassifier`, this is possible, as it is not very long to\n",
    "fit a model.\n",
    "However, for larger models, specifically `CNN` or other neural networks, this metric soon\n",
    "becomes hard to calculate.\n",
    "\n",
    "## First exercices\n",
    "\n",
    "1. Why has the for-loop `for seed in range` an `errors=\"ignore\"` in the call to `drop`?\n",
    "1. Besides the runtime, what changes in the figures if you change the `num_examples_to_metric`?\n",
    "1. Same question with regard to the `num_ref_models`?\n",
    "\n",
    "Once you're done, change the `verbose_reference` to `False` and save the notebook.\n",
    "\n",
    "## Further exercices\n",
    "\n",
    "1. Optimize the method `get_reference_metric` to only calculate one `out_models` for\n",
    "every `example_to_metric` of the test cases.\n",
    "This is possible because the `out_models` are trained using all training cases except\n",
    "the one from the `examples_to_metric`.\n",
    "And because all test cases will have the same `out_models`, it's enough to calculate this\n",
    "model only once.\n",
    "1. Create a table of `classification ROC` / `reference metric ROC` figures for different values\n",
    "of the parameters of `RandomForestClassifier`\n",
    "\n",
    "## NN exercices - no support\n",
    "\n",
    "Use the methods from the `ml_privacy_meter` to calculate the reference metric of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641023f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run 1-ml_load_data.ipynb\n",
    "verbose_reference = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a0efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reference_metric(train_model, X_train, y_train, X_test, y_test, \n",
    "                         num_examples_to_metric = 50, num_ref_models = 10):\n",
    "    \n",
    "    # Collect some arbitrary target examples to measure.\n",
    "    examples_to_metric = []\n",
    "\n",
    "    # ...half from the training data.\n",
    "    for index in X_train.index[:num_examples_to_metric // 2]:\n",
    "        examples_to_metric.append((index, X_train.loc[index], y_train.loc[index], 1))\n",
    "\n",
    "    # ...half from the test data.\n",
    "    for index in X_test.index[:num_examples_to_metric // 2]:\n",
    "        examples_to_metric.append((index, X_test.loc[index], y_test.loc[index], 0))\n",
    "\n",
    "    result = []\n",
    "    target_model = train_model(X_train, y_train)\n",
    "\n",
    "    # Now run the re-training metrics!\n",
    "    for index, x, y, is_member in tqdm(examples_to_metric):\n",
    "        # First, train a bunch of models without the target example \n",
    "        # (if it is in fact part of the training data)\n",
    "        out_models = []\n",
    "        for seed in range(num_ref_models):\n",
    "            ref_model = train_model(\n",
    "                X_train.drop(index=[index], errors=\"ignore\"),\n",
    "                y_train.drop(index=[index], errors=\"ignore\"),\n",
    "                seed=seed\n",
    "            )\n",
    "            out_models.append(ref_model)\n",
    "\n",
    "        # Compute the metric features.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            pred_in = target_model.predict_proba([x])\n",
    "            preds_out = [model.predict_proba([x])[0] for model in out_models]\n",
    "\n",
    "        logit_in = logit_scale(y, pred_in)\n",
    "        logits_out = logit_scale(y, preds_out)\n",
    "\n",
    "        # Next, we run a parametric test. We assume that \"out\" logits are \n",
    "        # Gaussian-distributed, so compute their mean and variance.\n",
    "        logits_out_mean = np.mean(logits_out)\n",
    "        logits_out_var = np.var(logits_out)\n",
    "\n",
    "        # The parametric test is computing the probability that the \"out\" logits are \n",
    "        # less than \"in\" logit, which means that we predict the target record as a member:\n",
    "        # \n",
    "        #   Pr[logit_out <= logit_in], where logit_out ~ Normal(mean, var) with mean and var\n",
    "        #   estimated from reference models.\n",
    "        #\n",
    "        # See https://arxiv.org/abs/2112.03570, Eq. (4)\n",
    "        prob = stats.norm(logits_out_mean, logits_out_var).cdf(logit_in) \n",
    "\n",
    "        result.append(dict(\n",
    "            target_index=index,\n",
    "            is_member=is_member,\n",
    "            prob=prob,\n",
    "        ))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d03bc5-170e-4f75-a5d0-ea828ff7c3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_reference_metric(train_model, X_train, y_train, X_test, y_test, num_examples = 20,\n",
    "                        num_ref_models = 10):\n",
    "    reference_metric = get_reference_metric(train_model, X_train, y_train, X_test, y_test, num_examples,\n",
    "                                           num_ref_models)\n",
    "    plot_rfc_auroc(pd.DataFrame(reference_metric).is_member, pd.DataFrame(reference_metric).prob,\n",
    "                  \"Reference metric\")\n",
    "\n",
    "if verbose_reference:\n",
    "    plot_reference_metric(train_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0251cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
