{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494e9cc6",
   "metadata": {},
   "source": [
    "## Measuring True Privacy Leakage\n",
    "\n",
    "### Simple exercices\n",
    "### Further exercices\n",
    "\n",
    "#### Optimize `get_reference_attack`\n",
    "\n",
    "Optimize the method `get_reference_attack` to only calculate one `out_models` for\n",
    "every `example_to_attack` of the test cases.\n",
    "This is possible because the `out_models` are trained using all training cases except\n",
    "the one from the `examples_to_attack`.\n",
    "And because all test cases will have the same `out_models`, it's enough to calculate this\n",
    "model only once.\n",
    "\n",
    "### Advanced exercices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641023f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ml_load_data.ipynb\n",
    "verbose_reference = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a0efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reference_attack(train_model, X_train, y_train, X_test, y_test, \n",
    "                         num_examples_to_attack = 50, num_ref_models = 10):\n",
    "    \n",
    "    # Collect some arbitrary target examples to attack.\n",
    "    examples_to_attack = []\n",
    "\n",
    "    # ...half from the training data.\n",
    "    for index in X_train.index[:num_examples_to_attack // 2]:\n",
    "        examples_to_attack.append((index, X_train.loc[index], y_train.loc[index], 1))\n",
    "\n",
    "    # ...half from the test data.\n",
    "    for index in X_test.index[:num_examples_to_attack // 2]:\n",
    "        examples_to_attack.append((index, X_test.loc[index], y_test.loc[index], 0))\n",
    "\n",
    "    result = []\n",
    "    target_model = train_model(X_train, y_train)\n",
    "\n",
    "    # Now run the re-training attacks!\n",
    "    for index, x, y, is_member in tqdm(examples_to_attack):\n",
    "        # First, train a bunch of models without the target example \n",
    "        # (if it is in fact part of the training data)\n",
    "        out_models = []\n",
    "        for seed in range(num_ref_models):\n",
    "            ref_model = train_model(\n",
    "                X_train.drop(index=[index], errors=\"ignore\"),\n",
    "                y_train.drop(index=[index], errors=\"ignore\"),\n",
    "                seed=seed\n",
    "            )\n",
    "            out_models.append(ref_model)\n",
    "\n",
    "        # Compute the attack features.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            pred_in = target_model.predict_proba([x])\n",
    "            preds_out = [model.predict_proba([x])[0] for model in out_models]\n",
    "\n",
    "        logit_in = logit_scale(y, pred_in)\n",
    "        logits_out = logit_scale(y, preds_out)\n",
    "\n",
    "        # Next, we run a parametric test. We assume that \"out\" logits are \n",
    "        # Gaussian-distributed, so compute their mean and variance.\n",
    "        logits_out_mean = np.mean(logits_out)\n",
    "        logits_out_var = np.var(logits_out)\n",
    "\n",
    "        # The parametric test is computing the probability that the \"out\" logits are \n",
    "        # less than \"in\" logit, which means that we predict the target record as a member:\n",
    "        # \n",
    "        #   Pr[logit_out <= logit_in], where logit_out ~ Normal(mean, var) with mean and var\n",
    "        #   estimated from reference models.\n",
    "        #\n",
    "        # See https://arxiv.org/abs/2112.03570, Eq. (4)\n",
    "        prob = stats.norm(logits_out_mean, logits_out_var).cdf(logit_in) \n",
    "\n",
    "        result.append(dict(\n",
    "            target_index=index,\n",
    "            is_member=is_member,\n",
    "            prob=prob,\n",
    "        ))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920bd46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_reference_attack(train_model, X_train, y_train, X_test, y_test, num_examples = 20):\n",
    "    reference_attack = get_reference_attack(train_model, X_train, y_train, X_test, y_test, num_examples)\n",
    "    plot_rfc_auroc(pd.DataFrame(reference_attack).is_member, pd.DataFrame(reference_attack).prob,\n",
    "                  \"Reference attack leakage\")\n",
    "\n",
    "if verbose_reference:\n",
    "    plot_reference_attack(train_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ee293-5e2b-46c8-8ab4-a5966028dbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
